<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>UMAP on sparse data &mdash; umap 0.5 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="_static/sg_gallery.css?v=61a4c737" />
      <link rel="stylesheet" type="text/css" href="_static/sg_gallery-binder.css?v=f4aeca0c" />
      <link rel="stylesheet" type="text/css" href="_static/sg_gallery-dataframe.css?v=2082cf3c" />
      <link rel="stylesheet" type="text/css" href="_static/sg_gallery-rendered-html.css?v=1277b6f3" />

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="_static/jquery.js?v=5d32c60e"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js?v=1c40f30e"></script>
        <script src="_static/doctools.js?v=888ff710"></script>
        <script src="_static/sphinx_highlight.js?v=4825356b"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.10/require.min.js"></script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="UMAP for Supervised Dimension Reduction and Metric Learning" href="supervised.html" />
    <link rel="prev" title="Parametric (neural network) Embedding" href="parametric_umap.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html">
            
              <img src="_static/logo.png" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                0.5
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">User Guide / Tutorial:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="basic_usage.html">How to Use UMAP</a></li>
<li class="toctree-l1"><a class="reference internal" href="parameters.html">Basic UMAP Parameters</a></li>
<li class="toctree-l1"><a class="reference internal" href="plotting.html">Plotting UMAP results</a></li>
<li class="toctree-l1"><a class="reference internal" href="reproducibility.html">UMAP Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="transform.html">Transforming New Data with UMAP</a></li>
<li class="toctree-l1"><a class="reference internal" href="inverse_transform.html">Inverse transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="parametric_umap.html">Parametric (neural network) Embedding</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">UMAP on sparse data</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#a-mathematical-example">A mathematical example</a></li>
<li class="toctree-l2"><a class="reference internal" href="#a-text-analysis-example">A text analysis example</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="supervised.html">UMAP for Supervised Dimension Reduction and Metric Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="clustering.html">Using UMAP for Clustering</a></li>
<li class="toctree-l1"><a class="reference internal" href="outliers.html">Outlier detection using UMAP</a></li>
<li class="toctree-l1"><a class="reference internal" href="composing_models.html">Combining multiple UMAP models</a></li>
<li class="toctree-l1"><a class="reference internal" href="densmap_demo.html">Better Preserving Local Density with DensMAP</a></li>
<li class="toctree-l1"><a class="reference internal" href="mutual_nn_umap.html">Improving the Separation Between Similar Classes Using a Mutual k-NN Graph</a></li>
<li class="toctree-l1"><a class="reference internal" href="document_embedding.html">Document embedding using UMAP</a></li>
<li class="toctree-l1"><a class="reference internal" href="embedding_space.html">Embedding to non-Euclidean spaces</a></li>
<li class="toctree-l1"><a class="reference internal" href="aligned_umap_basic_usage.html">How to use AlignedUMAP</a></li>
<li class="toctree-l1"><a class="reference internal" href="aligned_umap_politics_demo.html">AlignedUMAP for Time Varying Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="precomputed_k-nn.html">Precomputed k-nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="benchmarking.html">Performance Comparison of Dimension Reduction Implementations</a></li>
<li class="toctree-l1"><a class="reference internal" href="release_notes.html">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="faq.html">Frequently Asked Questions</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Background on UMAP:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="how_umap_works.html">How UMAP Works</a></li>
<li class="toctree-l1"><a class="reference internal" href="performance.html">Performance Comparison of Dimension Reduction Implementations</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Examples of UMAP usage</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="interactive_viz.html">Interactive Visualizations</a></li>
<li class="toctree-l1"><a class="reference internal" href="exploratory_analysis.html">Exploratory Analysis of Interesting Datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="scientific_papers.html">Scientific Papers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Reference:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="api.html">UMAP API Guide</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">umap</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">UMAP on sparse data</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/sparse.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="umap-on-sparse-data">
<h1>UMAP on sparse data<a class="headerlink" href="#umap-on-sparse-data" title="Permalink to this heading"></a></h1>
<p>Sometimes datasets get very large, and potentially very very high
dimensional. In many such cases, however, the data itself is sparse –
that is, while there are many many features, any given sample has only a
small number of non-zero features observed. In such cases the data can
be represented much more efficiently in terms of memory usage by a
sparse matrix data structure. It can be hard to find dimension reduction
techniques that work directly on such sparse data – often one applies a
basic linear technique such as <code class="docutils literal notranslate"><span class="pre">TruncatedSVD</span></code> from sklearn (which does
accept sparse matrix input) to get the data in a format amenable to
other more advanced dimension reduction techniques. In the case of UMAP
this is not necessary – UMAP can run directly on sparse matrix input.
This tutorial will walk through a couple of examples of doing this.
First we’ll need some libraries loaded. We need <code class="docutils literal notranslate"><span class="pre">numpy</span></code> obviously, but
we’ll also make use of <code class="docutils literal notranslate"><span class="pre">scipy.sparse</span></code> which provides sparse matrix
data structures. One of our examples will be purely mathematical, and
we’ll make use of <code class="docutils literal notranslate"><span class="pre">sympy</span></code> for that; the other example is test based
and we’ll use sklearn for that (specifically
<code class="docutils literal notranslate"><span class="pre">sklearn.feature_extraction.text</span></code>). Beyond that we’ll need umap, and
plotting tools.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">scipy.sparse</span>
<span class="kn">import</span> <span class="nn">sympy</span>
<span class="kn">import</span> <span class="nn">sklearn.datasets</span>
<span class="kn">import</span> <span class="nn">sklearn.feature_extraction.text</span>
<span class="kn">import</span> <span class="nn">umap</span>
<span class="kn">import</span> <span class="nn">umap.plot</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
</pre></div>
</div>
<section id="a-mathematical-example">
<h2>A mathematical example<a class="headerlink" href="#a-mathematical-example" title="Permalink to this heading"></a></h2>
<p>Our first example constructs a sparse matrix of data out of pure math.
This example is inspired by the work of <a class="reference external" href="https://johnhw.github.io/umap_primes/index.md.html">John
Williamson</a>, and
if you haven’t looked at that work you are strongly encouraged to do so.
The dataset under consideration will be the integers. We will represent
each integer by a vector of its divisibility by distinct primes. Thus
our feature space is the space of prime numbers (less than or equal to
the largest integer we will be considering) – potentially very high
dimensional. In practice a given integer is divisible by only a small
number of distinct primes, so each sample will be mostly made up of
zeros (all the primes that the number is not divisible by), and thus we
will have a very sparse dataset.</p>
<p>To get started we’ll need a list of all the primes. Fortunately we have
<code class="docutils literal notranslate"><span class="pre">sympy</span></code> at our disposal and we can quickly get that information with a
single call to <code class="docutils literal notranslate"><span class="pre">primerange</span></code>. We’ll also need a dictionary mapping the
different primes to the column number they correspond to in our data
structure; effectively we’ll just be enumerating the primes.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="n">primes</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">sympy</span><span class="o">.</span><span class="n">primerange</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">110000</span><span class="p">))</span>
<span class="n">prime_to_column</span> <span class="o">=</span> <span class="p">{</span><span class="n">p</span><span class="p">:</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">primes</span><span class="p">)}</span>
</pre></div>
</div>
<p>Now we need to construct our data in a format we can put into a sparse
matrix easily. At this point a little background on sparse matrix data
structures is useful. For this purpose we’ll be using the so called
<a class="reference external" href="https://scipy-lectures.org/advanced/scipy_sparse/lil_matrix.html">“LIL”
format</a>.
LIL is short for “List of Lists”, since that is how the data is
internally stored. There is a list of all the rows, and each row is
stored as a list giving the column indices of the non-zero entries. To
store the data values there is a parallel structure containing the value
of the entry corresponding to a given row and column.</p>
<p>To put the data together in this sort of format we need to construct
such a list of lists. We can do that by iterating over all the integers
up to a fixed bound, and for each integer (i.e. each row in our dataset)
generating the list of column indices which will be non-zero. The column
indices will simply be the indices corresponding to the primes that
divide the number. Since <code class="docutils literal notranslate"><span class="pre">sympy</span></code> has a function <code class="docutils literal notranslate"><span class="pre">primefactors</span></code> which
returns a list of the unique prime factors of any integer we simply need
to map those through our dictionary to covert the primes into column
numbers.</p>
<p>Parallel to that we’ll construct the corresponding structure of values
to insert into a matrix. Since we are only concerned with divisibility
this will simply be a one in every non-zero entry, so we can just add a
list of ones of the appropriate length for each row.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="n">time</span>
<span class="n">lil_matrix_rows</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">lil_matrix_data</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100000</span><span class="p">):</span>
    <span class="n">prime_factors</span> <span class="o">=</span> <span class="n">sympy</span><span class="o">.</span><span class="n">primefactors</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="n">lil_matrix_rows</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">prime_to_column</span><span class="p">[</span><span class="n">p</span><span class="p">]</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">prime_factors</span><span class="p">])</span>
    <span class="n">lil_matrix_data</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">prime_factors</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">CPU</span> <span class="n">times</span><span class="p">:</span> <span class="n">user</span> <span class="mf">2.07</span> <span class="n">s</span><span class="p">,</span> <span class="n">sys</span><span class="p">:</span> <span class="mf">26.4</span> <span class="n">ms</span><span class="p">,</span> <span class="n">total</span><span class="p">:</span> <span class="mf">2.1</span> <span class="n">s</span>
<span class="n">Wall</span> <span class="n">time</span><span class="p">:</span> <span class="mf">2.1</span> <span class="n">s</span>
</pre></div>
</div>
<p>Now we need to get that into a sparse matrix. Fortunately the
<code class="docutils literal notranslate"><span class="pre">scipy.sparse</span></code> package makes this easy, and we’ve already built the
data in a fairly useful structure. First we create a sparse matrix of
the correct format (LIL) and the right shape (as many rows as we have
generated, and as many columns as there are primes). This is essentially
just an empty matrix however. We can fix that by setting the <code class="docutils literal notranslate"><span class="pre">rows</span></code>
attribute to be the rows we have generated, and the <code class="docutils literal notranslate"><span class="pre">data</span></code> attribute
to be the corresponding structure of values (all ones). The result is a
sparse matrix data structure which can then be easily manipulated and
converted into other sparse matrix formats easily.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="n">factor_matrix</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">sparse</span><span class="o">.</span><span class="n">lil_matrix</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">lil_matrix_rows</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">primes</span><span class="p">)),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">factor_matrix</span><span class="o">.</span><span class="n">rows</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">lil_matrix_rows</span><span class="p">)</span>
<span class="n">factor_matrix</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">lil_matrix_data</span><span class="p">)</span>
<span class="n">factor_matrix</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">&lt;</span><span class="mi">100000</span><span class="n">x10453</span> <span class="n">sparse</span> <span class="n">matrix</span> <span class="n">of</span> <span class="nb">type</span> <span class="s1">&#39;&lt;class &#39;</span><span class="n">numpy</span><span class="o">.</span><span class="n">float32</span><span class="s1">&#39;&gt;&#39;</span>
    <span class="k">with</span> <span class="mi">266398</span> <span class="n">stored</span> <span class="n">elements</span> <span class="ow">in</span> <span class="n">LInked</span> <span class="n">List</span> <span class="nb">format</span><span class="o">&gt;</span>
</pre></div>
</div>
<p>As you can see we have a matrix with 100000 rows and over 10000 columns.
If we were storing that as a numpy array it would take a great deal of
memory. In practice, however, there are only 260000 or so entries that
are not zero, and that’s all we really need to store, making it much
more compact.</p>
<p>The question now is how can we feed that sparse matrix structure into
UMAP to have it learn an embedding. The answer is surprisingly
straightforward – we just hand it directly to the fit method. Just like
other sklearn estimators that can handle sparse input UMAP will detect
the sparse matrix and just do the right thing.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="n">time</span>
<span class="n">mapper</span> <span class="o">=</span> <span class="n">umap</span><span class="o">.</span><span class="n">UMAP</span><span class="p">(</span><span class="n">metric</span><span class="o">=</span><span class="s1">&#39;cosine&#39;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">low_memory</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">factor_matrix</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">CPU</span> <span class="n">times</span><span class="p">:</span> <span class="n">user</span> <span class="mi">9</span><span class="nb">min</span> <span class="mi">36</span><span class="n">s</span><span class="p">,</span> <span class="n">sys</span><span class="p">:</span> <span class="mf">6.76</span> <span class="n">s</span><span class="p">,</span> <span class="n">total</span><span class="p">:</span> <span class="mi">9</span><span class="nb">min</span> <span class="mi">43</span><span class="n">s</span>
<span class="n">Wall</span> <span class="n">time</span><span class="p">:</span> <span class="mi">9</span><span class="nb">min</span> <span class="mi">7</span><span class="n">s</span>
</pre></div>
</div>
<p>That was easy! But is it really working? We can easily plot the results:</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="n">umap</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">points</span><span class="p">(</span><span class="n">mapper</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">100000</span><span class="p">),</span> <span class="n">theme</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">)</span>
</pre></div>
</div>
<img alt="_images/sparse_11_1.png" src="_images/sparse_11_1.png" />
<p>And this looks very much in line with the results <a class="reference external" href="https://johnhw.github.io/umap_primes/index.md.html">John Williamson
got</a> with the
proviso that we only used 100,000 integers instead of 1,000,000 to
ensure that most users should be able to run this example (the full
million may require a large memory compute node). So it seems like this
is working well. The next question is whether we can use the
<code class="docutils literal notranslate"><span class="pre">transform</span></code> functionality to map new data into this space. To test
that we’ll need some more data. Fortunately there are more integers.
We’ll grab the next 10,000 and put them in a sparse matrix, much as we
did for the first 100,000.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="n">time</span>
<span class="n">lil_matrix_rows</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">lil_matrix_data</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100000</span><span class="p">,</span> <span class="mi">110000</span><span class="p">):</span>
    <span class="n">prime_factors</span> <span class="o">=</span> <span class="n">sympy</span><span class="o">.</span><span class="n">primefactors</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="n">lil_matrix_rows</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">prime_to_column</span><span class="p">[</span><span class="n">p</span><span class="p">]</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">prime_factors</span><span class="p">])</span>
    <span class="n">lil_matrix_data</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">prime_factors</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">CPU</span> <span class="n">times</span><span class="p">:</span> <span class="n">user</span> <span class="mi">214</span> <span class="n">ms</span><span class="p">,</span> <span class="n">sys</span><span class="p">:</span> <span class="mf">1.99</span> <span class="n">ms</span><span class="p">,</span> <span class="n">total</span><span class="p">:</span> <span class="mi">216</span> <span class="n">ms</span>
<span class="n">Wall</span> <span class="n">time</span><span class="p">:</span> <span class="mi">222</span> <span class="n">ms</span>
</pre></div>
</div>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="n">new_data</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">sparse</span><span class="o">.</span><span class="n">lil_matrix</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">lil_matrix_rows</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">primes</span><span class="p">)),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">new_data</span><span class="o">.</span><span class="n">rows</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">lil_matrix_rows</span><span class="p">)</span>
<span class="n">new_data</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">lil_matrix_data</span><span class="p">)</span>
<span class="n">new_data</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">&lt;</span><span class="mi">10000</span><span class="n">x10453</span> <span class="n">sparse</span> <span class="n">matrix</span> <span class="n">of</span> <span class="nb">type</span> <span class="s1">&#39;&lt;class &#39;</span><span class="n">numpy</span><span class="o">.</span><span class="n">float32</span><span class="s1">&#39;&gt;&#39;</span>
    <span class="k">with</span> <span class="mi">27592</span> <span class="n">stored</span> <span class="n">elements</span> <span class="ow">in</span> <span class="n">LInked</span> <span class="n">List</span> <span class="nb">format</span><span class="o">&gt;</span>
</pre></div>
</div>
<p>To map the new data we generated we can simply hand it to the
<code class="docutils literal notranslate"><span class="pre">transform</span></code> method of our trained model. This is a little slow, but it
does work.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="n">new_data_embedding</span> <span class="o">=</span> <span class="n">mapper</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">new_data</span><span class="p">)</span>
</pre></div>
</div>
<p>And we can plot the results. Since we just got the locations of the
points this time (rather than a model) we’ll have to resort to
matplotlib for plotting.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">12</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">new_data_embedding</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">new_data_embedding</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10000</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xticks</span><span class="o">=</span><span class="p">[],</span> <span class="n">yticks</span><span class="o">=</span><span class="p">[],</span> <span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">);</span>
</pre></div>
</div>
<img alt="_images/sparse_18_0.png" src="_images/sparse_18_0.png" />
<p>The color scale is different in this case, but you can see that the data
has been mapped into locations corresponding to the various structures
seen in the original embedding. Thus, even with large sparse data we can
embed the data, and even add new data to the embedding.</p>
</section>
<section id="a-text-analysis-example">
<h2>A text analysis example<a class="headerlink" href="#a-text-analysis-example" title="Permalink to this heading"></a></h2>
<p>Let’s look at a more classical machine learning example of working with
sparse high dimensional data – working with text documents. Machine
learning on text is hard, and there is a great deal of literature on the
subject, but for now we’ll just consider a basic approach. Part of the
difficulty of machine learning with text is turning language into
numbers, since numbers are really all most machine learning algorithms
understand (at heart anyway). One of the most straightforward ways to do
this for documents is what is known as the <a class="reference external" href="https://en.wikipedia.org/wiki/Bag-of-words_model">“bag-of-words”
model</a>. In this
model we view a document as simply a multi-set of the words contained in
it – we completely ignore word order. The result can be viewed as a
matrix of data by setting the feature space to be the set of all words
that appear in any document, and a document is represented by a vector
where the value of the <em>i</em>th entry is the number of times the <em>i</em>th
word occurs in that document. This is a very common approach, and is
what you will get if you apply sklearn’s <code class="docutils literal notranslate"><span class="pre">CountVectorizer</span></code> to a text
dataset for example. The catch with this approach is that the feature
space is often <em>very</em> large, since we have a feature for each and every
word that ever occurs in the entire corpus of documents. The data is
sparse however, since most documents only use a small portion of the
total possible vocabulary. Thus the default output format of
<code class="docutils literal notranslate"><span class="pre">CountVectorizer</span></code> (and other similar feature extraction tools in
sklearn) is a <code class="docutils literal notranslate"><span class="pre">scipy.sparse</span></code> format matrix.</p>
<p>For this example we’ll make use of the classic 20newsgroups dataset, a
sampling of newsgroup messages from the old NNTP newsgroup system
covering 20 different newsgroups. The <code class="docutils literal notranslate"><span class="pre">sklearn.datasets</span></code> module can
easily fetch the data, and, in fact, we can fetch a pre-vectorized
version to save us the trouble of running <code class="docutils literal notranslate"><span class="pre">CountVectorizer</span></code> ourselves.
We’ll grab both the training set, and the test set for later use.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="n">news_train</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">fetch_20newsgroups_vectorized</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="s1">&#39;train&#39;</span><span class="p">)</span>
<span class="n">news_test</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">fetch_20newsgroups_vectorized</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="s1">&#39;test&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>If we look at the actual data we have pulled back, we’ll see that
sklearn has run a <code class="docutils literal notranslate"><span class="pre">CountVectorizer</span></code> and produced the data in the sparse
matrix format.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="n">news_train</span><span class="o">.</span><span class="n">data</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">&lt;</span><span class="mi">11314</span><span class="n">x130107</span> <span class="n">sparse</span> <span class="n">matrix</span> <span class="n">of</span> <span class="nb">type</span> <span class="s1">&#39;&lt;class &#39;</span><span class="n">numpy</span><span class="o">.</span><span class="n">float64</span><span class="s1">&#39;&gt;&#39;</span>
    <span class="k">with</span> <span class="mi">1787565</span> <span class="n">stored</span> <span class="n">elements</span> <span class="ow">in</span> <span class="n">Compressed</span> <span class="n">Sparse</span> <span class="n">Row</span> <span class="nb">format</span><span class="o">&gt;</span>
</pre></div>
</div>
<p>The value of the sparse matrix format is immediately obvious in this
case; while there are only 11,000 samples there are 130,000 features! If
the data were stored in a standard <code class="docutils literal notranslate"><span class="pre">numpy</span></code> array we would be using up
10GB of memory! And most of that memory would simply be storing the
number zero, over and over again. In the sparse matrix format it easily fits
in memory on most machines. This sort of dimensionality of data is very
common with text workloads.</p>
<p>The raw counts are, however, not ideal since common words such as “the” and
“and” will dominate the counts for most documents, while contributing
very little information about the actual content of the document. We can
correct for this by using a <code class="docutils literal notranslate"><span class="pre">TfidfTransformer</span></code> from sklearn, which
will convert the data into <a class="reference external" href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf">TF-IDF
format</a>. There are lots
of ways to think about the transformation done by TF-IDF, but I like to
think of it intuitively as follows. The information content of a word
can be thought of as (roughly) proportional to the negative log of the
frequency of the word; the more often a word is used, the less
information it tends to carry, and infrequent words carry more
information. What TF-IDF is going to do can be thought of as akin to
re-weighting the columns according to the information content of the
word associated to that column. Thus the common words like “the” and
“and” will get down-weighted, as carrying less information about the
document, while infrequent words will be deemed more imporant and have
their associated columns up-weighted. We can apply this transformation
to both the train and test sets (using the same transformer trained on
the training set).</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="n">tfidf</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">feature_extraction</span><span class="o">.</span><span class="n">text</span><span class="o">.</span><span class="n">TfidfTransformer</span><span class="p">(</span><span class="n">norm</span><span class="o">=</span><span class="s1">&#39;l1&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">news_train</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
<span class="n">train_data</span> <span class="o">=</span> <span class="n">tfidf</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">news_train</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
<span class="n">test_data</span> <span class="o">=</span> <span class="n">tfidf</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">news_test</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
<p>The result is still a sparse matrix, since TF-IDF doesn’t change the
zero elements at all, nor the number of features.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_data</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">&lt;</span><span class="mi">11314</span><span class="n">x130107</span> <span class="n">sparse</span> <span class="n">matrix</span> <span class="n">of</span> <span class="nb">type</span> <span class="s1">&#39;&lt;class &#39;</span><span class="n">numpy</span><span class="o">.</span><span class="n">float64</span><span class="s1">&#39;&gt;&#39;</span>
    <span class="k">with</span> <span class="mi">1787565</span> <span class="n">stored</span> <span class="n">elements</span> <span class="ow">in</span> <span class="n">Compressed</span> <span class="n">Sparse</span> <span class="n">Row</span> <span class="nb">format</span><span class="o">&gt;</span>
</pre></div>
</div>
<p>Now we need to pass this very high dimensional data to UMAP. Unlike some
other non-linear dimension reduction techniques we don’t need to apply
PCA first to get the data down to a reasonable dimensionality; nor do we
need to use other techniques to reduce the data to be able to be
represented as a dense <code class="docutils literal notranslate"><span class="pre">numpy</span></code> array; we can work directly on the
130,000 dimensional sparse matrix.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%</span><span class="n">time</span>
<span class="n">mapper</span> <span class="o">=</span> <span class="n">umap</span><span class="o">.</span><span class="n">UMAP</span><span class="p">(</span><span class="n">metric</span><span class="o">=</span><span class="s1">&#39;hellinger&#39;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">CPU</span> <span class="n">times</span><span class="p">:</span> <span class="n">user</span> <span class="mi">8</span><span class="nb">min</span> <span class="mi">40</span><span class="n">s</span><span class="p">,</span> <span class="n">sys</span><span class="p">:</span> <span class="mf">3.07</span> <span class="n">s</span><span class="p">,</span> <span class="n">total</span><span class="p">:</span> <span class="mi">8</span><span class="nb">min</span> <span class="mi">44</span><span class="n">s</span>
<span class="n">Wall</span> <span class="n">time</span><span class="p">:</span> <span class="mi">8</span><span class="nb">min</span> <span class="mi">43</span><span class="n">s</span>
</pre></div>
</div>
<p>Now we can plot the results, with labels according to the target
variable of the data – which newsgroup the posting was drawn from.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="n">umap</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">points</span><span class="p">(</span><span class="n">mapper</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">news_train</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
</pre></div>
</div>
<img alt="_images/sparse_31_1.png" src="_images/sparse_31_1.png" />
<p>We can see that even going directly from a 130,000 dimensional space
down to only 2 dimensions UMAP has done a decent job of separating out
many of the different newsgroups.</p>
<p>We can now attempt to add the test data to the same space using the
<code class="docutils literal notranslate"><span class="pre">transform</span></code> method.</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="n">test_embedding</span> <span class="o">=</span> <span class="n">mapper</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">test_data</span><span class="p">)</span>
</pre></div>
</div>
<p>While this is somewhat expensive computationally, it does work, and we
can plot the end result:</p>
<div class="highlight-python3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">12</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">test_embedding</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">test_embedding</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">news_test</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;Spectral&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xticks</span><span class="o">=</span><span class="p">[],</span> <span class="n">yticks</span><span class="o">=</span><span class="p">[]);</span>
</pre></div>
</div>
<img alt="_images/sparse_35_0.png" src="_images/sparse_35_0.png" />
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="parametric_umap.html" class="btn btn-neutral float-left" title="Parametric (neural network) Embedding" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="supervised.html" class="btn btn-neutral float-right" title="UMAP for Supervised Dimension Reduction and Metric Learning" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2018, Leland McInnes.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>